{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rapidsnopp/MNIST-CNN/blob/main/Bai_3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a95d788-0f87-4999-a4cf-4d2e8350d093",
      "metadata": {
        "id": "8a95d788-0f87-4999-a4cf-4d2e8350d093"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb402401-3bbc-4b0a-bcff-2ee5ec19518b",
      "metadata": {
        "id": "fb402401-3bbc-4b0a-bcff-2ee5ec19518b"
      },
      "outputs": [],
      "source": [
        "def padding(x, padding: int | tuple | list, padding_type='zero'):\n",
        "    assert padding_type in ['zero', 'reflect', 'edge', 'symmetric']\n",
        "    if isinstance(padding, int):\n",
        "        padding = (padding, padding)\n",
        "    if padding_type == 'zero':\n",
        "        return np.pad(x, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])),\n",
        "                      'constant', constant_values=0)\n",
        "    else:\n",
        "        return np.pad(x, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), padding_type)\n",
        "\n",
        "def dilation(x, dilation: int | tuple | list):\n",
        "    if isinstance(dilation, int):\n",
        "        dilation = (dilation, dilation)\n",
        "    if dilation == 0:\n",
        "        return x\n",
        "    size = x.shape\n",
        "    h_out = size[2] + (size[2]-1) * dilation[0]\n",
        "    w_out = size[3] + (size[3]-1) * dilation[1]\n",
        "    out = np.zeros((size[0], size[1], h_out, w_out))\n",
        "    pad = dilation[0] + 1\n",
        "    for i in range(size[2]):\n",
        "        for j in range(size[3]):\n",
        "            out[:, :, i*pad, j*pad] = x[:, :, i, j]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e98577e-7193-40cd-9b10-6d8dfc589611",
      "metadata": {
        "id": "9e98577e-7193-40cd-9b10-6d8dfc589611"
      },
      "outputs": [],
      "source": [
        "class Conv2d:\n",
        "    def __init__(self, in_channels, out_channels, kernel_size: int | list | tuple, stride=1, padding=0, padding_type='edge', is_bias=True):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        assert isinstance(kernel_size, (int, list, tuple))\n",
        "        if isinstance(kernel_size, (tuple, list)):\n",
        "            assert len(kernel_size) == 2\n",
        "            self.kernel_size = kernel_size\n",
        "        else:\n",
        "            self.kernel_size = (kernel_size, kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.weights = np.random.randn(out_channels, in_channels, self.kernel_size[0], self.kernel_size[1])\n",
        "        self.bias = np.random.randn(out_channels) if is_bias else None\n",
        "        self.padding_type = padding_type\n",
        "        self.training = True\n",
        "        self.input = None\n",
        "        self.w_grad = None\n",
        "        self.b_grad = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "        x = padding(x, self.padding, self.padding_type)\n",
        "        if self.training:\n",
        "            self.input = x\n",
        "        batch_size, in_channels, height, width = x.shape\n",
        "        assert in_channels == self.in_channels\n",
        "        out = self._calculate_conv2d(x, self.weights, self.bias)\n",
        "        return out\n",
        "\n",
        "    def _calculate_conv2d(self, x, kernel, bias=None):\n",
        "        batch, in_channels, height, width = x.shape\n",
        "        out_channels, _, k_h, k_w = kernel.shape\n",
        "\n",
        "        out_height = (height - k_h) // self.stride + 1\n",
        "        out_width = (width - k_w) // self.stride + 1\n",
        "\n",
        "        size = [batch, out_channels, out_height, out_width]\n",
        "        out = np.zeros(size)\n",
        "\n",
        "        kernel = np.expand_dims(kernel, axis=0)\n",
        "        input_exp = np.expand_dims(x, axis=1)\n",
        "\n",
        "        kernel = np.repeat(kernel, repeats=size[0], axis=0)\n",
        "        input_exp = np.repeat(input_exp, repeats=size[1], axis=1)\n",
        "\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                h = i * self.stride\n",
        "                w = j * self.stride\n",
        "                out[:, :, i, j] = np.sum(input_exp[:, :, :, h:h+k_h, w:w+k_w] * kernel, axis=(-1, -2, -3))\n",
        "        if bias is not None:\n",
        "            bias = bias.reshape(1, self.out_channels, 1, 1)\n",
        "            out += bias\n",
        "        return out\n",
        "\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        assert self.input is not None\n",
        "        x = np.transpose(self.input, (1, 0, 2, 3))\n",
        "        kernel_back_w = np.transpose(grad_output, (1, 0, 2, 3))\n",
        "        kernel_back_w = dilation(kernel_back_w, self.stride-1)\n",
        "        w_grad = self._calculate_conv2d(x, kernel_back_w)\n",
        "        w_grad = np.transpose(w_grad, (1, 0, 2, 3))\n",
        "        b_grad = np.sum(grad_output, axis=(0, 2, 3))\n",
        "        self.w_grad = w_grad\n",
        "        self.b_grad = b_grad\n",
        "        w = np.transpose(self.weights, (1, 0, 2, 3))\n",
        "        dilation_out = dilation(grad_output, self.stride-1)\n",
        "        padding_grad_output = padding(dilation_out, (w.shape[2]-1, w.shape[3]-1))\n",
        "        grad_input = self._calculate_conv2d(padding_grad_output, w[:, :, ::-1, ::-1])\n",
        "        if self.padding > 0:\n",
        "            grad_input = grad_input[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        return grad_input\n",
        "\n",
        "    def update(self, lr):\n",
        "        if self.w_grad is None or self.b_grad is None:\n",
        "            return\n",
        "        self.weights -= lr * self.w_grad\n",
        "        if self.bias is not None:\n",
        "            self.bias -= lr * self.b_grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.w_grad = None\n",
        "        self.b_grad = None\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5998033-1871-4da5-9c3c-c32afb382212",
      "metadata": {
        "id": "f5998033-1871-4da5-9c3c-c32afb382212"
      },
      "outputs": [],
      "source": [
        "class MaxPool2d:\n",
        "    def __init__(self, kernel_size: int | list | tuple, stride=2, padding=0, padding_type='edge'):\n",
        "        if isinstance(kernel_size, (tuple, list)):\n",
        "            assert len(kernel_size) == 2\n",
        "            self.kernel_size = kernel_size\n",
        "        else:\n",
        "            self.kernel_size = (kernel_size, kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.padding_type = padding_type\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 3:\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "        x = padding(x, self.padding, self.padding_type)\n",
        "        self.input = x\n",
        "        out = self._calculate_maxpool2d(x)\n",
        "        return out\n",
        "\n",
        "    def _calculate_maxpool2d(self, x):\n",
        "        batch, in_channels, height, width = x.shape\n",
        "        out_height = (height - self.kernel_size[0]) // self.stride + 1\n",
        "        out_width = (width - self.kernel_size[1]) // self.stride + 1\n",
        "        size = [batch, in_channels, out_height, out_width]\n",
        "        out = np.zeros(size)\n",
        "        for i in range(out_height):\n",
        "            for j in range(out_width):\n",
        "                h = i * self.stride\n",
        "                w = j * self.stride\n",
        "                out[:, :, i, j] = np.max(x[:, :, h:h+self.kernel_size[0], w:w+self.kernel_size[1]], axis=(-1, -2))\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        assert self.input is not None\n",
        "        x = self.input\n",
        "        grad_input = np.zeros(x.shape)\n",
        "        size = grad_output.shape\n",
        "        for i in range(size[2]):\n",
        "            for j in range(size[3]):\n",
        "                h = i * self.stride\n",
        "                w = j * self.stride\n",
        "                x_slice = x[:, :, h:h+self.kernel_size[0], w:w+self.kernel_size[1]]\n",
        "                max_val = np.max(x_slice, axis=(-1, -2), keepdims=True)\n",
        "                mask = (x_slice == max_val).astype(int)\n",
        "                grad = grad_output[:, :, i, j][:, :, None, None]\n",
        "                grad_input[:, :, h:h+self.kernel_size[0], w:w+self.kernel_size[1]] += mask * grad\n",
        "        if self.padding > 0:\n",
        "            grad_input = grad_input[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        return grad_input\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "    def update(self, lr):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d67d537-f297-458a-806f-d836806c25c3",
      "metadata": {
        "id": "6d67d537-f297-458a-806f-d836806c25c3"
      },
      "outputs": [],
      "source": [
        "class Linear:\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        self.w = np.random.randn(in_channels, out_channels) * 0.01\n",
        "        self.b = np.zeros(out_channels)\n",
        "        self.training = True\n",
        "        self.input = None\n",
        "        self.w_grad = None\n",
        "        self.b_grad = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training:\n",
        "            self.input = x\n",
        "        return np.dot(x, self.w) + self.b\n",
        "\n",
        "    def backward(self, out_grad):\n",
        "        x = self.input\n",
        "        self.w_grad = np.dot(x.T, out_grad)\n",
        "        self.b_grad = np.sum(out_grad, axis=0)\n",
        "        return np.dot(out_grad, self.w.T)\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.w -= lr * self.w_grad\n",
        "        self.b -= lr * self.b_grad\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.w_grad = np.zeros_like(self.w)\n",
        "        self.b_grad = np.zeros_like(self.b)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "    def train(self):\n",
        "        self.training = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891b2e42-ee35-44aa-a2fd-6483f8ceb6b3",
      "metadata": {
        "id": "891b2e42-ee35-44aa-a2fd-6483f8ceb6b3"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation\n",
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def backward(self, out_grad):\n",
        "        sig = self.forward(self.input)\n",
        "        return out_grad * sig * (1 - sig)\n",
        "\n",
        "    def update(self, lr):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "# Tanh activattion\n",
        "class Tanh:\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    return np.tanh(x)\n",
        "\n",
        "  def backward(self, out_grad):\n",
        "    tanh = self.forward(self.input)\n",
        "    return out_grad * (1 - tanh**2)\n",
        "\n",
        "  def update(self, lr):\n",
        "    pass\n",
        "\n",
        "  def zero_grad(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def eval(self):\n",
        "    pass\n",
        "\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "# ReLU activation\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, out_grad):\n",
        "        return out_grad * (self.input > 0)\n",
        "\n",
        "    def update(self, lr):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass\n",
        "# Leaky ReLU activaation\n",
        "class LeakyReLU:\n",
        "    def __init__(self, alpha=0.1):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        return np.maximum(self.alpha*x, x)\n",
        "\n",
        "    def backward(self, out_grad):\n",
        "        return out_grad * (self.input > 0) + self.alpha * out_grad\n",
        "\n",
        "    def update(self, lr):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass\n",
        "\n",
        "# Maxout activation\n",
        "class Maxout:\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    return np.max(x.reshape(x.shape[0], x.shape[1] // 2, 2, x.shape[2], x.shape[3]), axis=2)\n",
        "\n",
        "  def backward(self, out_grad):\n",
        "    return out_grad\n",
        "\n",
        "  def update(self, lr):\n",
        "    pass\n",
        "\n",
        "  def zero_grad(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def eval(self):\n",
        "    pass\n",
        "\n",
        "  def train(self):\n",
        "    pass\n",
        "\n",
        "\n",
        "# ELU activation\n",
        "class ELU:\n",
        "  def __init__(self, alpha=0.1):\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    return np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
        "\n",
        "  def backward(self, out_grad):\n",
        "    return np.where(self.input > 0, out_grad, out_grad * self.alpha * np.exp(self.input))\n",
        "\n",
        "  def update(self, lr):\n",
        "    pass\n",
        "\n",
        "  def zero_grad(self):\n",
        "    pass\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.forward(x)\n",
        "\n",
        "  def eval(self):\n",
        "    pass\n",
        "\n",
        "  def train(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4bb71b1-1fe8-43ea-9b54-e2bd5d92b86d",
      "metadata": {
        "id": "f4bb71b1-1fe8-43ea-9b54-e2bd5d92b86d"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Đảm bảo y_pred không có giá trị quá 0\n",
        "        y_pred = np.clip(y_pred, 1e-7, 1)\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        # Khi kết hợp với softmax, đạo hàm của loss là (y_pred - y_true)\n",
        "        return y_pred - y_true\n",
        "\n",
        "    def __call__(self, y_pred, y_true):\n",
        "        return self.forward(y_pred, y_true)\n",
        "\n",
        "class SGD:\n",
        "    def __init__(self, model, lr=0.01):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        self.model.update(self.lr)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.model.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e1a2c9a-9f62-42d0-8020-f71b0866d02c",
      "metadata": {
        "id": "0e1a2c9a-9f62-42d0-8020-f71b0866d02c"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        x = x - np.max(x, axis=1, keepdims=True)\n",
        "        exp = np.exp(x)\n",
        "        self.output = exp / np.sum(exp, axis=1, keepdims=True)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        return y_pred - y_true\n",
        "\n",
        "    def update(self, lr):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bde2071-350a-402d-aaab-210b302daaeb",
      "metadata": {
        "id": "4bde2071-350a-402d-aaab-210b302daaeb"
      },
      "outputs": [],
      "source": [
        "class Sequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        for layer in self.layers[::-1]:\n",
        "            grad_output = layer.backward(grad_output)\n",
        "        return grad_output\n",
        "\n",
        "    def update(self, lr):\n",
        "        for layer in self.layers:\n",
        "            layer.update(lr)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for layer in self.layers:\n",
        "            layer.zero_grad()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        for layer in self.layers:\n",
        "            layer.eval()\n",
        "\n",
        "    def train(self):\n",
        "        for layer in self.layers:\n",
        "            layer.train()\n",
        "\n",
        "class Flatten:\n",
        "    def forward(self, x):\n",
        "        self.input_shape = x.shape\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        return grad_output.reshape(self.input_shape)\n",
        "\n",
        "    def update(self, lr):\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def eval(self):\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d95875-4e67-4a96-b25c-8aa1e7894260",
      "metadata": {
        "id": "e4d95875-4e67-4a96-b25c-8aa1e7894260"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock:\n",
        "    \"\"\"\n",
        "    Khối Residual gồm 2 lớp Conv2d với ReLU và thêm skip connection.\n",
        "    Nếu số kênh của input và output không khớp, thực hiện phép chiếu (projection).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_type='edge'):\n",
        "        self.conv1 = Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_type)\n",
        "        self.relu1 = ReLU()\n",
        "        self.conv2 = Conv2d(out_channels, out_channels, kernel_size, 1, padding, padding_type)\n",
        "        self.relu2 = ReLU()\n",
        "        # Nếu số kênh không khớp hoặc stride khác 1, dùng projection\n",
        "        if in_channels != out_channels or stride != 1:\n",
        "            self.proj = Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, padding_type=padding_type)\n",
        "        else:\n",
        "            self.proj = None\n",
        "        # Lưu lại các giá trị cần dùng cho backward\n",
        "        self.add = None\n",
        "        self.input = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        if self.proj is not None:\n",
        "            identity = self.proj(x)\n",
        "        else:\n",
        "            identity = x\n",
        "        self.add = out + identity\n",
        "        out = self.relu2(self.add)\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        # Backward qua ReLU cuối cùng\n",
        "        grad_relu2 = grad_output * (self.add > 0)\n",
        "        # Tách gradient cho 2 nhánh: qua conv2 và qua skip connection\n",
        "        grad_conv2 = grad_relu2.copy()\n",
        "        grad_identity = grad_relu2.copy()\n",
        "        grad_conv2 = self.conv2.backward(grad_conv2)\n",
        "        grad_conv2 = self.relu1.backward(grad_conv2)\n",
        "        grad_conv2 = self.conv1.backward(grad_conv2)\n",
        "        if self.proj is not None:\n",
        "            grad_identity = self.proj.backward(grad_identity)\n",
        "        grad_input = grad_conv2 + grad_identity\n",
        "        return grad_input\n",
        "\n",
        "    def update(self, lr):\n",
        "        self.conv1.update(lr)\n",
        "        self.conv2.update(lr)\n",
        "        if self.proj is not None:\n",
        "            self.proj.update(lr)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.conv1.zero_grad()\n",
        "        self.conv2.zero_grad()\n",
        "        if self.proj is not None:\n",
        "            self.proj.zero_grad()\n",
        "\n",
        "    def eval(self):\n",
        "        self.conv1.eval()\n",
        "        self.conv2.eval()\n",
        "        if self.proj is not None:\n",
        "            self.proj.eval()\n",
        "\n",
        "    def train(self):\n",
        "        self.conv1.train()\n",
        "        self.conv2.train()\n",
        "        if self.proj is not None:\n",
        "            self.proj.train()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e63a0a-7a4e-464f-b14a-242f73bac664",
      "metadata": {
        "id": "08e63a0a-7a4e-464f-b14a-242f73bac664"
      },
      "outputs": [],
      "source": [
        "class MNISTModel:\n",
        "    def __init__(self, in_channels, nums_classes, size, cfg=None):\n",
        "        self.conv_layer, out_size = self._make_conv_layer(in_channels, size, cfg)\n",
        "        self.fc_layer = self._make_fc_layer(out_size, nums_classes)\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "    def _make_conv_layer(self, in_channels, size, cfg=None):\n",
        "        # Nếu không có cấu hình, chỉ trả về đầu vào ban đầu\n",
        "        if cfg is None:\n",
        "            return None, (in_channels, size[0], size[1])\n",
        "        scale = 1\n",
        "        conv_layer = []\n",
        "        for layer in cfg:\n",
        "            if isinstance(layer, (list, tuple)):\n",
        "                # Cấu hình dạng [out_channels, kernel_size, stride, padding]\n",
        "                conv_layer.append(Conv2d(in_channels, layer[0], layer[1], layer[2], layer[3]))\n",
        "                in_channels = layer[0]\n",
        "            elif isinstance(layer, str):\n",
        "                if layer == 'M':\n",
        "                    conv_layer.append(MaxPool2d(2, 2))\n",
        "                    scale *= 2\n",
        "                # Sử dụng 'Res' để đánh dấu ResidualBlock (skip connection)\n",
        "                elif layer in ['R', 'Res']:\n",
        "                    conv_layer.append(ResidualBlock(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
        "        return Sequential(conv_layer), (in_channels, size[0] // scale, size[1] // scale)\n",
        "\n",
        "    def _make_fc_layer(self, out_size, nums_classes):\n",
        "        return Sequential([\n",
        "            Flatten(),\n",
        "            Linear(out_size[0] * out_size[1] * out_size[2], nums_classes),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.conv_layer is not None:\n",
        "            x = self.conv_layer(x)\n",
        "        x = self.fc_layer(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        grad_output = y_pred - y_true\n",
        "        grad_output = self.fc_layer.backward(grad_output)\n",
        "        if self.conv_layer is not None:\n",
        "            grad_output = self.conv_layer.backward(grad_output)\n",
        "        return grad_output\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def update(self, lr):\n",
        "        if self.conv_layer is not None:\n",
        "            self.conv_layer.update(lr)\n",
        "        self.fc_layer.update(lr)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        if self.conv_layer is not None:\n",
        "            self.conv_layer.zero_grad()\n",
        "        self.fc_layer.zero_grad()\n",
        "\n",
        "    def eval(self):\n",
        "        if self.conv_layer is not None:\n",
        "            self.conv_layer.eval()\n",
        "        self.fc_layer.eval()\n",
        "\n",
        "    def train(self):\n",
        "        if self.conv_layer is not None:\n",
        "            self.conv_layer.train()\n",
        "        self.fc_layer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc5311b-5a72-4214-b509-32709983e051",
      "metadata": {
        "id": "1fc5311b-5a72-4214-b509-32709983e051"
      },
      "outputs": [],
      "source": [
        "class MNISTDataset:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X.reshape(-1, 1, 28, 28)\n",
        "        self.y = self._convert_to_one_hot(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def _convert_to_one_hot(self, y):\n",
        "        one_hot = np.zeros((len(y), 10))\n",
        "        one_hot[np.arange(len(y)), y.astype(int)] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class Dataloader:\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.n_batches = len(dataset) // batch_size\n",
        "        if len(dataset) % batch_size != 0:\n",
        "            self.n_batches += 1\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(self.n_batches):\n",
        "            batch_X = self.dataset.X[i*self.batch_size : (i+1)*self.batch_size]\n",
        "            batch_y = self.dataset.y[i*self.batch_size : (i+1)*self.batch_size]\n",
        "            yield batch_X, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "645bb48a-057d-4d0e-b416-55ea27f07163",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "645bb48a-057d-4d0e-b416-55ea27f07163",
        "outputId": "4b305ddb-9699-4cf3-c7ab-0bece38af327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Loss: 0.44105750285131706\n",
            "Epoch 2/100 | Loss: 0.2747682769137471\n",
            "Epoch 3/100 | Loss: 0.23926126647038604\n",
            "Epoch 4/100 | Loss: 0.21729543925859202\n",
            "Epoch 5/100 | Loss: 0.20159601863785423\n",
            "Epoch 6/100 | Loss: 0.18937839521398495\n",
            "Epoch 7/100 | Loss: 0.17930191387252326\n",
            "Epoch 8/100 | Loss: 0.17071974316008917\n",
            "Epoch 9/100 | Loss: 0.16324201967682772\n",
            "Epoch 10/100 | Loss: 0.1565666696307195\n",
            "Epoch 11/100 | Loss: 0.15057953011892175\n",
            "Epoch 12/100 | Loss: 0.14516242673961552\n",
            "Epoch 13/100 | Loss: 0.1402210920165429\n",
            "Epoch 14/100 | Loss: 0.13565789860183944\n",
            "Epoch 15/100 | Loss: 0.1314402996268171\n",
            "Epoch 16/100 | Loss: 0.12751990525432724\n"
          ]
        }
      ],
      "source": [
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist.data, mnist.target\n",
        "X = X.to_numpy().astype(np.float32) / 255.0\n",
        "y = y.to_numpy()\n",
        "# zero center data\n",
        "X -= np.mean(X, axis=0)\n",
        "# normalized data\n",
        "X /= np.std(X, axis=0)\n",
        "\n",
        "test_size = 0.2\n",
        "X_train = X[2000:]\n",
        "y_train = y[2000:]\n",
        "X_test = X[:400]\n",
        "y_test = y[:400]\n",
        "\n",
        "train_dataset = MNISTDataset(X_train, y_train)\n",
        "test_dataset = MNISTDataset(X_test, y_test)\n",
        "\n",
        "# Cấu hình mạng với skip connection: ban đầu Conv2d, sau đó ResidualBlock, rồi MaxPool2d\n",
        "cfg = [\n",
        "    [4, 3, 1, 1],\n",
        "    'Res',    # ResidualBlock với skip connection\n",
        "    'M',\n",
        "]\n",
        "\n",
        "dataloader = Dataloader(train_dataset, 64)\n",
        "model = MNISTModel(1, 10, (28, 28), cfg)\n",
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = SGD(model, 0.00001)\n",
        "\n",
        "EPOCHS = 100\n",
        "losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    list_loss = []\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        y_pred = model(batch_X)\n",
        "        loss = loss_fn(y_pred, batch_y)\n",
        "        model.backward(y_pred, batch_y)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        list_loss.append(loss)\n",
        "    losses.append(sum(list_loss) / len(list_loss))\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} | Loss: {sum(list_loss)/len(list_loss)}')\n",
        "\n",
        "plt.plot(range(1, len(losses)+1), losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "model.eval()\n",
        "y_pred = model(X_test.reshape(-1, 1, 28, 28))\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.array(y_test).astype(int)\n",
        "print(\"Test Accuracy:\", np.mean(y_pred == y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}